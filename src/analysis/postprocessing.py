"""
Post processing analysis of the data extracted from the Google forms:
functions to extract relevant statistics and plots from the gathered data
"""

from IPython.display import display
import pandas as pd
import numpy as np
from tqdm import tqdm
from src.constants import LABEL_SIZE,TITLE_SIZE,COLOR1,COLOR2,EXPORT_DIR,EMOJIS, HONEYPOTS, EMOJI_2_TOP_INDEX_PATH, LANGUAGES_PATH, EMOJI_DATASET_DIR
import pickle as pk
from pdb import set_trace
from src.analysis.spelling import WordSuggester
import Levenshtein
from pathlib import Path
import matplotlib.pyplot as plt
import argparse
from src.analysis.fraudulous import filter_out, detect_honey_frauders, detect_repeat_frauders,study_outsiders
from src.utils import write_to_latex
import seaborn as sns
sns.set()

def get_emojis_voc_counts(path):
    """
    Generate a value count of words for every emoji present in the csv files
    found in the child directories of "path"

    Args:
        path (str): parent path of the csv files

    Return:
        em2vocab [dict of dict]: a dict associating each word to its count is mapped for each emoji
    """
    path = Path(path)
    em2vocab = {}
    for path in path.glob("**/[0-9]*.csv"):
        df = pd.read_csv(path)
        emojis = [col for col in df.columns if col in EMOJIS]
        for em in emojis:
            vocab = em2vocab.get(em, {})
            for word, count in df[em].value_counts().iteritems():
                pre_count = vocab.get(word, 0)
                pre_count += count
                vocab[word] = pre_count
            em2vocab[em] = vocab
    return em2vocab


def write_emojis_voc_counts(em2vocab, path):
    """
    write the dict of dict as generated by get_emojis_voc_counts

    Args:
        em2vocab (dict of dict): dict associating each word to its count is mapped for each emoji
        path (str/pathlib.Path): path where to write the file
    """
    with open(path, "w") as f:
        for em, wordcounts in em2vocab.items():
            f.write(em + "\n")
            # sort by count value
            wordcounts = {
                k: v
                for k, v in sorted(
                    wordcounts.items(), key=lambda item: item[1], reverse=True
                )
            }
            for word, count in wordcounts.items():
                f.write(f"\t{word}:{count}\n")


def display_whole_dir(directory):
    """
    display all df present in a subdirectory of directory

    Args:
        directory (pathlib.Path): path to the directory
    """
    for path in directory.glob("**/[0-9]*.csv"):
        df = pd.read_csv(path)
        display(df)


def parse_language(target_word, word_list):
    """
    Find the closest language match to target_word in terms of Levenshtein distance in word_list

    Args:
        target_word (str): user input for "mothertongue"
        word_list (str): list of official languages

    Returns:
        [str]: closest match / None if the levenshtein distance is >4
    """
    if target_word is None:
        return None
    if target_word == "en":
        return "english"
    distances = [(word, Levenshtein.distance(target_word, word)) for word in word_list]
    best_word, min_dist = min(distances, key=lambda x: x[1])
    check_pairs = [pair[0] for pair in distances if pair[1] == min_dist]
    if len(check_pairs) > 1:
        raise ValueError(f"Two words of the list are at equal distance! {check_pairs}")
    if min_dist > 4:
        return None
    return best_word


def build_worker_info_table(input_directory, verbose=False):
    """
    Gather and clean the demographic information about workers and save it in the output directory.

    Args:
        input_directory (str): parent directory in which the function will recursively find the csv files
    """
    input_directory = Path(input_directory)
    worker_infos = []
    for path in input_directory.glob("**/[0-9]*.csv"):
        df = pd.read_csv(path, usecols=["WorkerID", "Age", "Gender", "Mothertongue"])
        worker_infos.append(df)
    worker_infos = pd.concat(worker_infos, axis=0)

    worker_infos.drop_duplicates(inplace=True)

    worker_infos = worker_infos.groupby("WorkerID").first()

    languages = pd.read_csv(LANGUAGES_PATH)["0"].tolist()

    worker_infos["Mothertongue"] = worker_infos["Mothertongue"].apply(
        lambda word: parse_language(word, languages)
    )

    worker_infos["Mothertongue"].unique()

    # compute the amount of rows with missing entries
    N = worker_infos.shape[0]
    N_miss = worker_infos.isna().any(axis=1).sum()
    if verbose:
        print(f"Ratio of missing worker demographic info: {N_miss/N *100:.2f} %")
    return worker_infos


def scrap_form_results(dataset_dir):
    """
    Recursively sraps all [0-9]*.csv files in the subdirectories and return
    them concatenated in a list

    Args:
        dataset_dir (str): parent to directory to start scraping from

    Returns:
        [list of pd.DataFrame]: all form results with their respective metada associated
    """
    dataset_dir = Path(dataset_dir)
    worker_infos = build_worker_info_table(
            input_directory=dataset_dir, verbose=False
    )
    form_dfs = []
    paths = list(dataset_dir.glob("**/[0-9]*.csv"))
    for path in tqdm(paths):
        form_id = int(path.stem)
        df = pd.read_csv(path)
        df["FormId"] = form_id
        comp_time_path = path.parent.joinpath("completion_times.csv")
        comp_time = pd.read_csv(comp_time_path)

        # Recover information specific to this batch (completion time)
        df = pd.merge(df, comp_time, how="left", on=["WorkerID", "FormId"])

        # Recover demographic information about workers (Age, sex etc)
        df.drop(columns=["Age", "Gender", "Mothertongue", "Feedback"], inplace=True)
        df = pd.merge(df, worker_infos, how="left", on=["WorkerID"])

        form_dfs.append(df)
        if len(form_dfs) == 0:
            raise ValueError(f"No .csv file was found in the subdirectories of {path}")

    return form_dfs


def generate_production_format(form_dfs):
    """
    Generate the production format from a csv file/ a parent directory
    whose children contain csv files

    Args:
        form_dfs (list of pandas DataFrame): path to the csv file/parent directory

    Return:
        [pd.Dataframe]: the equivalent df(s) in production format
    """
    selem2indx = pk.load(open(EMOJI_2_TOP_INDEX_PATH, "rb"))
    data = []
    for df in tqdm(form_dfs):
        em_cols = [col for col in df.columns if col in EMOJIS]
        n = len(em_cols)
        honey_col_idx = n // 2 - (n + 1) % 2
        assert em_cols[honey_col_idx] in HONEYPOTS.keys()
        # get rid of the honeypot
        del em_cols[honey_col_idx]
        for _, row in df.iterrows():
            wid = row["WorkerID"]
            formid = row["FormId"]
            duration = row["AnswerDurationInSeconds"]
            for em, word in row[em_cols].iteritems():
                # we add the selected emojis index
                emoji_index = selem2indx[em]
                data.append((wid, formid, duration, emoji_index, em, word))
    data = pd.DataFrame(
        data, columns=["WorkerID", "FormId", "Duration", "emoji_index", "emoji", "word"]
    ).sort_values("emoji_index")
    return data

def get_varied_cstt_results(tot_df):
    voc_df = (tot_df.groupby('emoji')['word'].agg(lambda x: len(set(x)))
                .sort_values(ascending = False).index)
    varied_em_df = tot_df[tot_df['emoji'] == voc_df[0]].sample(5)
    cstt_em_df = tot_df[tot_df['emoji'] == voc_df[-3]].sample(5)
    return varied_em_df,cstt_em_df


def generate_dataset(input_dir, lshtein, voc_size):
    # Gather every form dataframe in a list
    form_dfs = scrap_form_results(input_dir)

    N = sum([df.shape[0] for df in form_dfs])
    print(f"Initial data shape: {N} rows")
    # Repeat outsiders
    if voc_size is not None:
        n_frauders, form_dfs = filter_out(
        form_dfs, detect_repeat_frauders, min_voc_size=voc_size, verbose=True, display_=False)

    # Honeypots outsiders
    if lshtein is not None:
        n_honey, form_dfs = filter_out(
        form_dfs, detect_honey_frauders, HONEYPOTS, dist_lshtein=lshtein, verbose=True)
    
    dataset_df = generate_production_format(form_dfs)
    sugg = WordSuggester()
    sugg.correct_prod_df(dataset_df)
    return dataset_df

def plot_nmb_form_per_worker(tot_df,ax=None,fig=None):
    if ax is None or fig is None:
        fig,ax = plt.subplots(1, figsize=(10, 5))
    form_per_worker = (tot_df.groupby('WorkerID')['FormId']
                             .agg(lambda x: len(set(x))))
    form_per_worker.hist(ax=ax,color=COLOR2)
    ax.set_xlabel("Number of form answered",fontsize=LABEL_SIZE)
    ax.set_ylabel("Number of workers",fontsize=LABEL_SIZE)
    ax.set_title("Histogram of number of forms answered per worker",fontsize=TITLE_SIZE)

def main():
    # Parameters
    input_dir = EMOJI_DATASET_DIR
    export_dir = EXPORT_DIR.joinpath("data/dataset")
    export_dir.mkdir(exist_ok=True,parents=True)
    lshtein = 3
    voc_size = 0.8

    # Dataset creation
    dataset_df = generate_dataset(input_dir,lshtein,voc_size)
    dataset_df.to_csv(export_dir.joinpath("emoji_dataset_prod.csv"),index=False)

    # Workers demographic information
    worker_infos = build_worker_info_table(input_directory=input_dir, verbose=True)
    worker_infos.to_csv(export_dir.joinpath("demographic_info.csv"))

    # Dataset Statistics
    export_dir = EXPORT_DIR.joinpath("report_files")
    export_dir.mkdir(exist_ok=True,parents=True)
    fig,axes = plt.subplots(1,2,figsize=(15,5))
    plot_hist_nmb_anot_per_emoji(dataset_df)
    plt.savefig(export_dir.joinpath("dataset_distribution.jpeg"))

    fig,ax = plt.subplots(1, figsize=(10, 5))
    plot_nmb_form_per_worker(dataset_df,fig=fig,ax=ax)
    plt.savefig(export_dir.joinpath("form_per_worker.jpeg"))

    # Save most varied, most constant and sample of the datset
    np.random.seed(14)
    export_dir = EXPORT_DIR.joinpath("report_files/")
    export_dir.mkdir(exist_ok=True,parents=True)
    var_df,cstt_df = get_varied_cstt_results(dataset_df)
    sample_df = dataset_df.sample(15)
    worker_sample = worker_infos.sample(5)

    write_to_latex(export_dir.joinpath("varied.tex"),var_df)
    write_to_latex(export_dir.joinpath("cstt.tex"),cstt_df)
    write_to_latex(export_dir.joinpath("sample.tex"),sample_df)
    write_to_latex(export_dir.joinpath("worker_sample.tex"),worker_sample,index=True)

def plot_hist_nmb_anot_per_emoji(tot_df,axes=None,fig=None):
    """
    """
    if axes is None or fig is None:
        fig,axes = plt.subplots(1,2,figsize=(15,5))

    ax = axes[0]
    val_counts = tot_df['emoji'].value_counts()
    val_counts.hist(ax=ax,color=COLOR1)
    ax.set_xlabel("Number of annotations",fontsize=LABEL_SIZE)
    ax.set_ylabel("Number of emojis",fontsize=LABEL_SIZE)
    ax.set_title("Histogram of annotations number per emojis",fontsize=TITLE_SIZE)

    ax = axes[1]
    voc_size_per_emoji = tot_df.groupby('emoji')['word'].agg(lambda x: len(set(x)))
    voc_size_per_emoji = voc_size_per_emoji / val_counts
    voc_size_per_emoji.hist(ax=ax,bins=25,color=COLOR1)
    ax.set_xlabel('Size of vocabulary/word',fontsize=LABEL_SIZE)
    ax.set_ylabel('Number of emojis',fontsize=LABEL_SIZE)
    ax.set_title('Histogram of normalized vocabulary size',fontsize=TITLE_SIZE)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-i", "--input_dir", help="Input dataset directory",
        default="/app/data/raw/forms/dataset"
    )
    parser.add_argument(
        "-v", "--voc_size", help="Minimal vocabulary size ratio",type=float, default=None
    )
    parser.add_argument(
        "-l", "--lshtein", help="Tolerance for the honeypots inputs in terms of Levenshtein distance",type=float, default=None
    )
    parser.add_argument(
        "-o", "--export_dir", help="Output path for the production format of the dataset",
        default="/app/results"
    )
    args = parser.parse_args()

    input_dir = args.input_dir
    export_dir = args.export_dir
    lshtein = args.lshtein
    voc_size = args.voc_size
    dataset_df = generate_dataset(input_dir, lshtein, voc_size)
    dataset_df.to_csv(export_dir.joinpath("emoji_dataset_prod.csv"))

