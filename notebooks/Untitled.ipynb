{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "from pdb import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quick brown fox jumps over the lazy dog, 8, -0.8021201856258288\n"
     ]
    }
   ],
   "source": [
    "sym_spell = SymSpell(max_dictionary_edit_distance=0, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "# term_index is the column of the term and count_index is the\n",
    "# column of the term frequency\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up down, 1, -0.9643268255583413\n"
     ]
    }
   ],
   "source": [
    "sym_spell = SymSpell(max_dictionary_edit_distance=0, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "# term_index is the column of the term and count_index is the\n",
    "# column of the term frequency\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "# a sentence without any spaces\n",
    "input_term = \"updown\"\n",
    "result = sym_spell.word_segmentation(input_term)\n",
    "print(\"{}, {}, {}\".format(result.corrected_string, result.distance_sum,\n",
    "                          result.log_prob_sum/len(result.corrected_string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up down, 1, -6.750287778908389\n"
     ]
    }
   ],
   "source": [
    "sym_spell = SymSpell(max_dictionary_edit_distance=0, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "# term_index is the column of the term and count_index is the\n",
    "# column of the term frequency\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "# a sentence without any spaces\n",
    "input_term = \"updown\"\n",
    "result = sym_spell.word_segmentation(input_term)\n",
    "print(\"{}, {}, {}\".format(result.corrected_string, result.distance_sum,\n",
    "                          result.log_prob_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.corrected_string.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordSuggester():\n",
    "    \n",
    "    def __init__(self,): \n",
    "        print(\"Initializing the vocabulary set..\")\n",
    "        self.word_set = set(brown.words())\n",
    "        print(\"Initializing BERT pipeline..\")\n",
    "\n",
    "        self.tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.bert = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "        self.sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "        self.sym_spell_cut = SymSpell(max_dictionary_edit_distance=0, prefix_length=7)\n",
    "        dictionary_path = pkg_resources.resource_filename(\n",
    "            \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "        # term_index is the column of the term and count_index is the\n",
    "        # column of the term frequency\n",
    "        self.sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "        self.sym_spell_cut.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "    def validate_word(self,word,word_counts,min_counts=2):\n",
    "        \"\"\"\n",
    "        A word is considered valid if it occures many times or \n",
    "        \"\"\"\n",
    "        tot = sum(word_counts.values())\n",
    "        return word_counts[word] >= min_counts    \n",
    "\n",
    "    def get_word_suggestions(self,word,word_counts):\n",
    "        \"\"\"\n",
    "        Return the suggestions for the word passed in parameter. If the \n",
    "        word passed in parameter is valid, return a list of len 1 with the\n",
    "        word inside.\n",
    "        \n",
    "        Args:\n",
    "            word (str): the word to find suggestions for\n",
    "            word_counts (dict): value counts of word for a given emoji (context)\n",
    "        \"\"\"\n",
    "        # if the word appears many times we keep it\n",
    "        if self.validate_word(word,word_counts):\n",
    "            return {'status':'present','words':[word]}\n",
    "        \n",
    "        # if the word is part of the vocabulary we keep it\n",
    "        if word in self.word_set:\n",
    "            return {'status':'exist','words':[word]}\n",
    "        \n",
    "        # if it is a combinaison of many words\n",
    "        result = self.sym_spell_cut.word_segmentation(word)\n",
    "        log_confidence = result.log_prob_sum/len(result.corrected_string)\n",
    "        if log_confidence > -1:\n",
    "            suggestions = result.corrected_string.split(\" \")\n",
    "            if len(suggestions) == 1:\n",
    "                return {'status':'exist','words':suggestions}\n",
    "            return {'status':'cut','words':suggestions}\n",
    "        \n",
    "        # otherwise we correct it\n",
    "        suggestions = self.sym_spell.lookup(word, Verbosity.CLOSEST,\n",
    "                                       max_edit_distance=2)\n",
    "        # display suggestion term, term frequency, and edit distance\n",
    "        if len(suggestions) == 0:\n",
    "            print(f\"Word {word} not found!\")\n",
    "            return {'status':'notfound','words':[word]}\n",
    "        return {'status':'corrected','words':[sugg.term for sugg in suggestions]}\n",
    "\n",
    "    def get_context_suggestions(self,word_list):\n",
    "        \"\"\"\n",
    "        Applies get_word_suggestions for every word of an emoji's vocabulary (context)\n",
    "        \n",
    "        Args:\n",
    "            word_list (list of str): words to describe the emoji\n",
    "        \n",
    "        Returns:\n",
    "            [list of list of str]: list of suggestions: each word receives suggestions (list of str)\n",
    "        \"\"\"\n",
    "        word_counts = Counter(word_list)\n",
    "        context_suggestions = [self.get_word_suggestions(word,word_counts) for word in word_list]\n",
    "        return context_suggestions\n",
    "    \n",
    "\n",
    "    def find_best_word(self,context,suggestions):\n",
    "        \"\"\"\n",
    "        Find the most appropriate word in suggestions given the context\n",
    "        \n",
    "        Args:\n",
    "            context (list of str): words defining the context\n",
    "            suggestions (list of str): suggestions for the word to find\n",
    "        \n",
    "        Returns:\n",
    "            [str]: the word of suggestions that matches the best the context\n",
    "            according to BERT output\n",
    "        \"\"\"\n",
    "        # We place the word of interest in the middle of the context\n",
    "        n = len(context) // 2\n",
    "        pre_context = ' '.join(context[:n])\n",
    "        post_context = ' '.join(context[n:])\n",
    "        sentence = f\"{pre_context} {self.tok.mask_token} {post_context}\"\n",
    "\n",
    "        input_tokens = self.tok.encode(sentence)\n",
    "        answer_pos = input_tokens.index(self.tok.mask_token_id)\n",
    "\n",
    "        logits = self.bert(torch.tensor([input_tokens]))[0][0]\n",
    "        logits = logits[answer_pos]\n",
    "        suggestions_tokens = [self.tok.encode(word)[1:-1] for word in suggestions]\n",
    "        scores = [np.mean([logits[i].item() for i in tokens]) for tokens in suggestions_tokens]\n",
    "        best_sugg_idx = np.argmax(scores)\n",
    "        return suggestions[best_sugg_idx]\n",
    "    \n",
    "    def extract_context_suggestions(self,context_suggestions):\n",
    "        \"\"\"\n",
    "        Extract best words for each suggestions in the context suggestions\n",
    "        \n",
    "        Args:\n",
    "            context_suggestions (list of list of str): list of suggestions\n",
    "        \n",
    "        Returns:\n",
    "            [list of str]: most appropriate words\n",
    "\n",
    "        \"\"\"\n",
    "        # we don't need the status in the current function\n",
    "        context_suggestions = [sugg['words'] for sugg in context_suggestions]\n",
    "        ret_words = []\n",
    "        for suggestions in context_suggestions:\n",
    "            # single suggestion: the word is not ambiguous\n",
    "            if len(suggestions) == 1:\n",
    "                ret_words.append(suggestions[0])\n",
    "            else:\n",
    "                # we gather the single words considered as healthy\n",
    "                context = [word_list[0] for word_list in context_suggestions\n",
    "                                         if word_list != suggestions and len(word_list) == 1 ]\n",
    "                word = self.find_best_word(context,suggestions)\n",
    "                \n",
    "                ret_words.append(word)\n",
    "        return ret_words\n",
    "    \n",
    "    def process_context(self,context,verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            context (list of str): words\n",
    "        \n",
    "        Returns:\n",
    "            [list of str]: corrected words\n",
    "        \"\"\"\n",
    "        context_suggestions = self.get_context_suggestions(context)\n",
    "        corr_words = self.extract_context_suggestions(context_suggestions)\n",
    "        if verbose:\n",
    "            for word,suggestions,corr_word in zip(context,context_suggestions,corr_words):\n",
    "                if suggestions['status'] not in ['present','exist']:\n",
    "                    status = suggestions['status']\n",
    "                    print(f\"Modified {word} --> {corr_word} ({status})\")\n",
    "        return corr_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the vocabulary set..\n",
      "Initializing BERT pipeline..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "sugg = WordSuggester()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected fruito --> fruit\n"
     ]
    }
   ],
   "source": [
    "voc = sugg.process_context([\"applette\",\"applette\",\"fruito\",\"sugary\",\"sweet\"],verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis.postprocessing import scrap_form_results,generate_production_format\n",
    "from src.constants import EMOJI_DATASET_DIR\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133/133 [00:51<00:00,  2.57it/s]\n"
     ]
    }
   ],
   "source": [
    "form_dfs = scrap_form_results(EMOJI_DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133/133 [00:03<00:00, 37.80it/s]\n"
     ]
    }
   ],
   "source": [
    "form_df = generate_production_format(form_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "form_df.word.apply(lambda x: \" \" in x).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_form_df = form_df.groupby('emoji')['word'].agg(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_form_df = small_form_df.iloc[:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emoji\n",
       "#️⃣    [number, hashtag, pound, number, pound, pound,...\n",
       "*️⃣    [asterisk, pound, snowflake, asterisk, asterik...\n",
       "©️     [copywrite, copyright, copywrite, copyright, c...\n",
       "®️     [registered, r, letter, copyright, r, rest, ra...\n",
       "‼️     [exclamation, exclamation, excited, exclamatio...\n",
       "⁉️     [confused, exclaim, seriously, surprised, ques...\n",
       "™️     [text, trademark, tm, trademark, tm, trademark...\n",
       "ℹ️     [information, i, hand, doubt, letter, exclamat...\n",
       "↔️     [sign, arrow, navigation, leftorright, turn, w...\n",
       "↕️     [directions, arrows, vertical, perpendicular, ...\n",
       "↖️     [diagonal, click, upleftarrow, diagonal, angle...\n",
       "↗️     [right, turn, up, up, arrow, arrow, up, right,...\n",
       "↘️     [direction, down, corner, down, down, downhear...\n",
       "↙️     [arrow, down, arrow, arrow, down, arrow, diago...\n",
       "Name: word, dtype: object"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_form_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 4/14 [00:00<00:00, 27.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified asterik --> asterisk (corrected)\n",
      "Modified astrik --> astrid (corrected)\n",
      "Modified coppyright --> copyright (corrected)\n",
      "Modified cee --> cen (corrected)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 7/14 [00:00<00:00,  9.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified capitalr --> capital (corrected)\n",
      "Modified icon --> icon (corrected)\n",
      "Modified circler --> circlet (corrected)\n",
      "Modified excalmation --> exclamation (corrected)\n",
      "Modified exclamationquestion --> question (cut)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 11/14 [00:01<00:00,  8.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified leftorright --> or (cut)\n",
      "Modified whichway --> way (cut)\n",
      "Modified leftright --> left (cut)\n",
      "Modified upanddown --> and (cut)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 13/14 [00:01<00:00,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified upleftarrow --> up (cut)\n",
      "Modified angled --> angled (corrected)\n",
      "Modified risingsign --> sign (cut)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:01<00:00,  7.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified arrowbottomleftcorner --> arrow (cut)\n",
      "Modified thatway --> way (cut)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "emoji\n",
       "#️⃣    [number, hashtag, pound, number, pound, pound,...\n",
       "*️⃣    [asterisk, pound, snowflake, asterisk, asteris...\n",
       "©️     [copywrite, copyright, copywrite, copyright, c...\n",
       "®️     [registered, r, letter, copyright, r, rest, ra...\n",
       "‼️     [exclamation, exclamation, excited, exclamatio...\n",
       "⁉️     [confused, exclaim, seriously, surprised, ques...\n",
       "™️     [text, trademark, tm, trademark, tm, trademark...\n",
       "ℹ️     [information, i, hand, doubt, letter, exclamat...\n",
       "↔️     [sign, arrow, navigation, or, turn, wider, dir...\n",
       "↕️     [directions, arrows, vertical, perpendicular, ...\n",
       "↖️     [diagonal, click, up, diagonal, angled, arrow,...\n",
       "↗️     [right, turn, up, up, arrow, arrow, up, right,...\n",
       "↘️     [direction, down, corner, down, down, downhear...\n",
       "↙️     [arrow, down, arrow, arrow, down, arrow, diago...\n",
       "Name: word, dtype: object"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_form_df.progress_apply(lambda x: sugg.process_context(x,verbose=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
